README
---


About TPC-H
===

TPC-H is a read-only workload of "analytics" queries on large datasets.

TPC-H datasets are scaled by a "scale factor", which is a multiplier of the
number of rows in each table.

Tables, and rows per table:

nation      25
region      5
part        200000*SF
supplier    10000*SF
partsupp    800000*SF
customer    150000*SF
orders      1500000*SF
lineitems   6001215*SF

At scale factor = 1, the data in raw text is about 1gb, and 5gb of data on disk
per node (in a 3 node cluster). For perspective, scale factor 300 is a common
test, and scale factor 100000 (100 terabytes) is the largest official TPC-H test.


Loading TPC-H data
===

In order to run the TPC-H loader, you will need to obtain the dataset
for it. There are a few ways to do so:

1. Acquire a cockroach backup file, which will load into a cluster
within minutes.
1a. Load the cockroach backup using the Enterprise backup/restore feature from the local filesystem.
1b. Load the cockroach backup using the Enterprise backup/restore feature from Azure blob store.
2. Acquire a set of "tbl" files of raw data, and load them into a
cluster in hours.
3. Generate data files yourself using the `dbgen` utility, and load
them into a cluster.

(1b) is the fastest and the recommended way - but requires access to
Cockroach's Azure blob store. Ask someone at Cockroach Labs for the
link, as we do pay Azure data egress fees on each download (not much,
but we won't post the link publicly here due to the possibility of a
DDOS). All it requires is running, in a cockroach cluster with the
backup/restore feature on:

    CREATE DATABASE tpch;

Followed by

    restore tpch.* from 'azure://backup-test/benchmarks/tpch/scalefactor-1?AZURE_ACCOUNT_NAME=cockroachbackuptest&AZURE_ACCOUNT_KEY=<ommitted>';

To start a cluster with the enteprise backup/restore feature, you
currently need to start the cluster with:

    COCKROACH_PROPOSER_EVALUATED_KV=true COCKROACH_ENTERPRISE_ENABLED=true ./cockroach start

(1a) is tricky: the files must first be copied to every node in the
cluster, in the same filesystem location. (Thus, it is simpler to load
the data into a 1-node cluster, then bring up all subsequent nodes and
allow cockroach to do the rebalancing for you). Run:

    ./tpch -restore=/path/to/backup

If for some reason you wish to follow this path instead of (1b), a
tarball is available behind a cockroachlabs.com authentication gate on
Google Drive:
https://drive.google.com/open?id=0B2yAkR0eFsMEQlNHekhlaE5VTXM


(2): To obtain a set of .tbl files for the raw data in the 8 tables,
and put them in the folder tpch/data, you will need to compile and run
the "dbgen" program distributed by the TPC. If you do not want to run
dbgen, a tarball of a set of tbl files (at scale factor 1) is
available on Google Drive at
https://drive.google.com/open?id=0B2yAkR0eFsMEYlJtOWJ0SWZrcTg (behind
a cockroachlabs.com authentication gate).

If you put the .tbl files inside a directory named `data` in the same
directory as the `tpch` binary, running ./tpch -load will load the
data into Cockroach. This currently takes a lot of time (~5 hours on a
gceworker).


TPCH-H Scalefactors
===

TPC-H data is parameterized by a "scalefactor". A scalefactor of 1
represents a dataset of roughly 1 gigabyte in size. The above
instructions and download files are for a scalefactor of 1. However,
TPC-H benchmarks are usually done with larger scalefactors, such as
100 or 300.

Running TPC-H queries
===

Currently, not all queries can be run, due to the use of features such
as correlated subqueries. Those queries that do run are set to run by
default. TPC-H query compatibility and ongoing performance work is
being tracked at cockroachdb/cockroach#14295.
