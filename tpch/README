README
---


About TPC-H
===

TPC-H is a read-only workload of "analytics" queries on large datasets.

TPC-H datasets are scaled by a "scale factor", which is a multiplier of the
number of rows in each table.

Tables, and rows per table:

nation      25
region      5
part        200000*SF
supplier    10000*SF
partsupp    800000*SF
customer    150000*SF
orders      1500000*SF
lineitems   6001215*SF

At scale factor = 1, the data in raw text is about 1gb, and 5gb of data on disk
per node (in a 3 node cluster). For perspective, scale factor 300 is a common
test, and scale factor 100000 (100 terabytes) is the largest official TPC-H test.

Loading TPC-H data
===

In order to run the TPC-H loader, you will need to obtain the dataset
for it. There are a few ways to do so:

1. Acquire a cockroach backup file, which will load into a cluster
within minutes and load the cockroach backup using the Enterprise
backup/restore feature from Azure blob store.
2. Acquire a set of "tbl" files of raw data, and load them into a
cluster in hours.
3. Generate data files yourself using the `dbgen` utility, and load
them into a cluster.
4. Load the cockroach backup using the Enterprise backup/restore
feature from the local filesystem (only recommended for single node
setups).

(1) is the fastest and the recommended way - but requires access to
Cockroach's Azure blob store. Ask someone at Cockroach Labs for the
link, as we do pay Azure data egress fees on each download (pennies,
but we don't post the link publicly here due to the possibility of a
DDOS). All it requires is running, in a cockroach cluster with the
backup/restore feature on:

    CREATE DATABASE tpch;

Followed by

    restore tpch.* from 'azure://backup-test/benchmarks/tpch/scalefactor-1?AZURE_ACCOUNT_NAME=cockroachbackuptest&AZURE_ACCOUNT_KEY=<omitted>';

To start a cluster with the enteprise backup/restore feature, you
currently need to start the cluster with:

    COCKROACH_PROPOSER_EVALUATED_KV=true COCKROACH_ENTERPRISE_ENABLED=true ./cockroach start

(2) and (3): To obtain a set of .tbl files for the raw data in the 8
tables, and put them in the folder tpch/data, you will need to compile
and run the "dbgen" program distributed by the TPC. If you do not want
to run dbgen, a tarball of a set of tbl files (at scale factor 1) is
available on Google Drive at
https://drive.google.com/open?id=0B2yAkR0eFsMEYlJtOWJ0SWZrcTg (behind
a cockroachlabs.com authentication gate).

If you put the .tbl files inside a directory named `data` in the same
directory as the `tpch` binary, running `./tpch -load` will load the
data into Cockroach. This currently takes a lot of time (several
multiples of the time using a RESTORE).

(4) is tricky: the files must first be copied to every node in the
cluster, in the same filesystem location. (Thus, this is really only
useful for one-node clusters). Run:

    ./tpch -restore=/path/to/backup <url-to-cluster>

If for some reason you wish to follow this path instead of (1), a
tarball is available behind a cockroachlabs.com authentication gate on
Google Drive:
https://drive.google.com/open?id=0B2yAkR0eFsMEQlNHekhlaE5VTXM


TPCH-H Scalefactors
===

TPC-H data is parameterized by a "scalefactor". A scalefactor of 1
represents a dataset of roughly 1 gigabyte in size. The above
instructions and download files are for a scalefactor of 1. However,
TPC-H benchmarks are usually done with larger scalefactors, such as
100 or 300.

Running TPC-H queries
===

Once you have a cockroach cluster with data loaded, you can run
`tpch`:

    ./tpch postgresql://root@localhost:26257/tpch?sslmode=disable

You can specify specific queries to run by specifying them as a comma
separated list of numbers to the `queries` flag (for example:
`-queries=1,2,3`)

If you are connecting to a secure cluster, you will need to provide
the certificates in the connection URL. For example, if your node
certificate is at `certs/node.crt`, your node key is at
`certs/node.key`, and your root certificate is at `certs/ca.crt`:

    ./tpch postgresql://root@localhost:26257/tpch?sslcert=certs%2Fnode.crt&sslkey=certs%2Fnode.key&sslmode=verify-full&sslrootcert=certs%2Fca.crt


Currently, not all queries can be run, due to the use of features such
as correlated subqueries. Those queries that do run are set to run by
default. TPC-H query compatibility and ongoing performance work is
being tracked at cockroachdb/cockroach#14295.
